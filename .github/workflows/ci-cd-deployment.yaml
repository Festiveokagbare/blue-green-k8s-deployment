name: CI/CD Blue-Green Deployment

on:
  push:
    branches:
      - main
      - 'release/*'
  workflow_dispatch:

env:
  APP_NAME: blue-green-app
  NAMESPACE: default
  GREEN_LABEL: green
  BLUE_LABEL: blue

jobs:

  # ========================================================================
  # BUILD AND SCAN (unchanged from your version)
  # ========================================================================
  build-and-scan:
    runs-on: ubuntu-latest
    outputs:
      blue_image: ${{ steps.push-blue.outputs.image }}
      green_image: ${{ steps.push-green.outputs.image }}

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: Install Bandit & deps
        run: |
          pip install --upgrade pip
          pip install bandit
          if [ -f app/blue/requirements.txt ]; then pip install -r app/blue/requirements.txt; fi
          if [ -f app/green/requirements.txt ]; then pip install -r app/green/requirements.txt; fi

      - name: Run Bandit Scan
        run: |
          bandit -r app/blue -ll || true
          bandit -r app/green -ll || true

      - name: Set up QEMU
        uses: docker/setup-qemu-action@v2

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v2

      - name: Login to Amazon ECR
        uses: aws-actions/amazon-ecr-login@v1
        with:
          region: us-east-1
        env:
          AWS_REGION: us-east-1
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}

      - name: Build & Push BLUE image
        id: push-blue
        uses: docker/build-push-action@v5
        with:
          context: ./app/blue
          file: ./app/blue/Dockerfile
          push: true
          tags: |
            ${{ secrets.ECR_REGISTRY }}/${{ secrets.ECR_REPOSITORY_BLUE }}:${{ github.sha }}
            ${{ secrets.ECR_REGISTRY }}/${{ secrets.ECR_REPOSITORY_BLUE }}:prod

      - name: Build & Push GREEN image
        id: push-green
        uses: docker/build-push-action@v5
        with:
          context: ./app/green
          file: ./app/green/Dockerfile
          push: true
          tags: |
            ${{ secrets.ECR_REGISTRY }}/${{ secrets.ECR_REPOSITORY_GREEN }}:${{ github.sha }}
            ${{ secrets.ECR_REGISTRY }}/${{ secrets.ECR_REPOSITORY_GREEN }}:prod

      - name: Trivy Scan (BLUE image)
        uses: aquasecurity/trivy-action@master
        with:
          image-ref: ${{ secrets.ECR_REGISTRY }}/${{ secrets.ECR_REPOSITORY_BLUE }}:${{ github.sha }}
          format: table
          exit-code: '1'
          severity: CRITICAL,HIGH

      - name: Trivy Scan (GREEN image)
        uses: aquasecurity/trivy-action@master
        with:
          image-ref: ${{ secrets.ECR_REGISTRY }}/${{ secrets.ECR_REPOSITORY_GREEN }}:${{ github.sha }}
          format: table
          exit-code: '1'
          severity: CRITICAL,HIGH

  # ========================================================================
  # DEPLOY, VERIFY, FLIP, AUTO-ROLLBACK
  # ========================================================================
  deploy-and-release:
    needs: build-and-scan
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main' || startsWith(github.ref, 'refs/heads/release/')
    env:
      AWS_REGION: ${{ secrets.AWS_REGION }}
      AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
      AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
      NAMESPACE: default
      APP_NAME: blue-green-app
      GREEN_LABEL: green
      BLUE_LABEL: blue

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v2
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ secrets.AWS_REGION }}

      # --------------------------
      # INSTALL AND CONFIGURE TOOLS
      # --------------------------
      - name: Install kubectl
        run: |
          curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"
          sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl
          kubectl version --client

      - name: Update AWS CLI (if needed)
        run: |
          pip install --upgrade awscli

      # --------------------------
      # EXTRACT CLUSTER INFO FROM EXISTING KUBECONFIG
      # --------------------------
      - name: Decode and analyze kubeconfig
        env:
          KUBE_CONFIG_DATA: ${{ secrets.KUBE_CONFIG_DATA }}
        run: |
          echo "$KUBE_CONFIG_DATA" | base64 --decode > /tmp/kubeconfig.yaml

          # Extract cluster name from kubeconfig
          # Method 1: Try to get cluster name from contexts
          CLUSTER_NAME=$(grep -A2 "contexts:" /tmp/kubeconfig.yaml | grep "cluster:" | head -1 | awk '{print $2}' | tr -d '\r' || true)

          # Method 2: Try to get cluster name from clusters section
          if [ -z "$CLUSTER_NAME" ]; then
            CLUSTER_NAME=$(grep "name:" /tmp/kubeconfig.yaml | grep -i "cluster" | head -1 | awk '{print $2}' | tr -d '\r' || true)
          fi

          # Method 3: Try to extract from server URL (EKS endpoint)
          if [ -z "$CLUSTER_NAME" ]; then
            SERVER_URL=$(grep "server:" /tmp/kubeconfig.yaml | head -1 | awk '{print $2}' | tr -d '\r' || true)
            if [[ $SERVER_URL == *"eks.amazonaws.com"* ]]; then
              CLUSTER_NAME=$(echo $SERVER_URL | sed -n 's|.*//\(.*\)\.gr7\.\(.*\)\.eks\.amazonaws\.com.*|\1|p' || true)
            fi
          fi

          # If still empty, use a default or ask user to set it
          if [ -z "$CLUSTER_NAME" ]; then
            echo "WARNING: Could not extract cluster name from kubeconfig"
            echo "Please set EKS_CLUSTER_NAME as a repository secret"
            echo "Using placeholder - YOU MUST SET EKS_CLUSTER_NAME SECRET"
            CLUSTER_NAME="YOUR_EKS_CLUSTER_NAME"
          fi

          echo "Extracted cluster name: $CLUSTER_NAME"
          echo "CLUSTER_NAME=$CLUSTER_NAME" >> $GITHUB_ENV

          # Save original kubeconfig for reference
          cp /tmp/kubeconfig.yaml /tmp/original-kubeconfig.yaml

      # --------------------------
      # GENERATE PROPER KUBECONFIG FOR EKS
      # --------------------------
      - name: Generate EKS kubeconfig
        run: |
          # Check if CLUSTER_NAME is set, otherwise use secret
          if [ "$CLUSTER_NAME" = "YOUR_EKS_CLUSTER_NAME" ] && [ -n "${{ secrets.EKS_CLUSTER_NAME }}" ]; then
            echo "Using cluster name from secret"
            CLUSTER_NAME="${{ secrets.EKS_CLUSTER_NAME }}"
            echo "CLUSTER_NAME=$CLUSTER_NAME" >> $GITHUB_ENV
          fi

          # Verify AWS credentials
          echo "Verifying AWS credentials..."
          aws sts get-caller-identity

          # Verify EKS cluster access
          echo "Checking EKS cluster..."
          aws eks describe-cluster --name $CLUSTER_NAME --region $AWS_REGION || {
            echo "ERROR: Cannot access EKS cluster '$CLUSTER_NAME' in region '$AWS_REGION'"
            echo "Please verify:"
            echo "1. Cluster name is correct"
            echo "2. AWS credentials have proper permissions"
            echo "3. Cluster exists in region $AWS_REGION"
            exit 1
          }

          # Generate kubeconfig
          echo "Generating kubeconfig for EKS cluster: $CLUSTER_NAME"
          aws eks update-kubeconfig \
            --name $CLUSTER_NAME \
            --region $AWS_REGION \
            --kubeconfig $HOME/kubeconfig

          # Set KUBECONFIG environment variable
          echo "KUBECONFIG=$HOME/kubeconfig" >> $GITHUB_ENV

          # Verify the generated kubeconfig
          echo "Generated kubeconfig contents:"
          kubectl config view --kubeconfig=$HOME/kubeconfig

      # --------------------------
      # VERIFY KUBERNETES ACCESS
      # --------------------------
      - name: Verify Kubernetes connectivity
        run: |
          echo "Testing kubectl connection..."

          # Test basic commands with retry
          for i in {1..5}; do
            echo "Attempt $i to connect to Kubernetes..."
            if kubectl cluster-info; then
              echo "✓ Successfully connected to Kubernetes"
              break
            else
              echo "✗ Connection failed, retrying in 5 seconds..."
              sleep 5
            fi
          done

          # Get nodes and namespaces
          echo "Listing nodes:"
          kubectl get nodes

          echo "Listing namespaces:"
          kubectl get ns

          echo "Current context:"
          kubectl config current-context

      # --------------------------
      # OPTIONAL: COMPARE WITH ORIGINAL CONFIG
      # --------------------------
      - name: Debug - Compare kubeconfigs
        if: ${{ false }}  # Set to true if you want to debug
        run: |
          echo "===== ORIGINAL KUBECONFIG ====="
          cat /tmp/original-kubeconfig.yaml || true
          echo ""
          echo "===== GENERATED KUBECONFIG ====="
          cat $HOME/kubeconfig || true

      - name: Ensure namespace exists
        run: |
          kubectl get ns ${{ env.NAMESPACE }} || kubectl create ns ${{ env.NAMESPACE }}

      # --------------------------
      # DEPLOY GREEN
      # --------------------------
      - name: Deploy GREEN version
        env:
          GREEN_IMAGE: ${{ secrets.REGISTRY }}/${{ secrets.IMAGE_NAMESPACE }}/green-app:${{ github.sha }}
        run: |
          sed -e "s|<GREEN_IMAGE>|${GREEN_IMAGE}|g" \
            k8s/deployments/green-deployment.yaml | kubectl apply -f -

      - name: Wait for GREEN rollout
        id: rollout
        run: |
          kubectl rollout status deployment/app-green \
            --namespace=${{ env.NAMESPACE }} \
            --timeout=180s

      # --------------------------
      # SMOKE TEST GREEN
      # --------------------------
      - name: Smoke test GREEN pod
        id: smoketest
        run: |
          set -e
          for i in {1..12}; do
            POD=$(kubectl get pods -n ${{ env.NAMESPACE }} \
              -l app=${{ env.APP_NAME }},version=${{ env.GREEN_LABEL }} \
              -o jsonpath="{.items[0].metadata.name}" || true)

            if [ -n "$POD" ]; then
              kubectl exec -n ${{ env.NAMESPACE }} $POD \
                -- curl -sSf http://localhost:5000/health && break
            fi

            echo "Waiting for GREEN pod readiness..."
            sleep 10
          done

      # --------------------------
      # FLIP SERVICE TO GREEN
      # --------------------------
      - name: Flip service to GREEN
        id: flip
        run: |
          kubectl -n ${{ env.NAMESPACE }} patch service app-service \
            -p '{"spec":{"selector":{"app":"blue-green-app","version":"green"}}}'

      # --------------------------
      # VERIFY GREEN SERVICE
      # --------------------------
      - name: Verify GREEN service
        id: verify
        run: |
          set -e
          for i in {1..10}; do
            kubectl get svc app-service -n ${{ env.NAMESPACE }}
            echo "Waiting for GREEN service response..."
            sleep 6

            POD=$(kubectl get pods -n ${{ env.NAMESPACE }} \
              -l app=${{ env.APP_NAME }},version=${{ env.GREEN_LABEL }} \
              -o jsonpath="{.items[0].metadata.name}" || true)

            if [ -n "$POD" ]; then
              kubectl exec -n ${{ env.NAMESPACE }} $POD \
                -- curl -sSf http://localhost:5000/health && exit 0
            fi
          done

          echo "GREEN service failed probe"
          exit 1


      # =====================================================================
      # AUTO-ROLLBACK if any step above failed
      # =====================================================================
      - name: AUTO-ROLLBACK to BLUE
        if: failure()
        run: |
          echo "⚠️ GREEN deployment failed — starting rollback..."

          echo "➡ Switching service back to BLUE"
          kubectl -n ${{ env.NAMESPACE }} patch service app-service \
            -p '{"spec":{"selector":{"app":"blue-green-app","version":"blue"}}}'

          echo "➡ Scaling GREEN to zero"
          kubectl scale deployment app-green --replicas=0 -n ${{ env.NAMESPACE }} || true

          echo "Rollback completed ❗ Deployment remains on BLUE"
          exit 1  # Mark workflow as failed so you know deployment failed


      # --------------------------
      # SCALE DOWN OLD BLUE
      # --------------------------
      - name: Scale down BLUE deployment
        if: success()
        run: |
          kubectl scale deployment app-blue --replicas=0 -n ${{ env.NAMESPACE }} || true

