name: CI/CD Blue-Green Deployment

on:
  push:
    branches:
      - main
      - 'release/*'
  workflow_dispatch:

env:
  APP_NAME: blue-green-app
  NAMESPACE: default
  GREEN_LABEL: green
  BLUE_LABEL: blue

jobs:

  # ========================================================================
  # BUILD AND SCAN (unchanged from your version)
  # ========================================================================
  build-and-scan:
    runs-on: ubuntu-latest
    outputs:
      blue_image: ${{ steps.push-blue.outputs.image }}
      green_image: ${{ steps.push-green.outputs.image }}

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: Install Bandit & deps
        run: |
          pip install --upgrade pip
          pip install bandit
          if [ -f app/blue/requirements.txt ]; then pip install -r app/blue/requirements.txt; fi
          if [ -f app/green/requirements.txt ]; then pip install -r app/green/requirements.txt; fi

      - name: Run Bandit Scan
        run: |
          bandit -r app/blue -ll || true
          bandit -r app/green -ll || true

      - name: Set up QEMU
        uses: docker/setup-qemu-action@v2

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v2

      - name: Login to Amazon ECR
        uses: aws-actions/amazon-ecr-login@v1
        with:
          region: us-east-1
        env:
          AWS_REGION: us-east-1
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}

      - name: Build & Push BLUE image
        id: push-blue
        uses: docker/build-push-action@v5
        with:
          context: ./app/blue
          file: ./app/blue/Dockerfile
          push: true
          tags: |
            ${{ secrets.ECR_REGISTRY }}/${{ secrets.ECR_REPOSITORY_BLUE }}:${{ github.sha }}
            ${{ secrets.ECR_REGISTRY }}/${{ secrets.ECR_REPOSITORY_BLUE }}:prod

      - name: Build & Push GREEN image
        id: push-green
        uses: docker/build-push-action@v5
        with:
          context: ./app/green
          file: ./app/green/Dockerfile
          push: true
          tags: |
            ${{ secrets.ECR_REGISTRY }}/${{ secrets.ECR_REPOSITORY_GREEN }}:${{ github.sha }}
            ${{ secrets.ECR_REGISTRY }}/${{ secrets.ECR_REPOSITORY_GREEN }}:prod

      - name: Trivy Scan (BLUE image)
        uses: aquasecurity/trivy-action@master
        with:
          image-ref: ${{ secrets.ECR_REGISTRY }}/${{ secrets.ECR_REPOSITORY_BLUE }}:${{ github.sha }}
          format: table
          exit-code: '1'
          severity: CRITICAL,HIGH

      - name: Trivy Scan (GREEN image)
        uses: aquasecurity/trivy-action@master
        with:
          image-ref: ${{ secrets.ECR_REGISTRY }}/${{ secrets.ECR_REPOSITORY_GREEN }}:${{ github.sha }}
          format: table
          exit-code: '1'
          severity: CRITICAL,HIGH

  # ========================================================================
  # DEPLOY, VERIFY, FLIP, AUTO-ROLLBACK
  # ========================================================================
  deploy-and-release:
    needs: build-and-scan
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main' || startsWith(github.ref, 'refs/heads/release/')
    env:
      AWS_REGION: ${{ secrets.AWS_REGION }}
      AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
      AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
      NAMESPACE: default
      APP_NAME: blue-green-app
      GREEN_LABEL: green
      BLUE_LABEL: blue

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v2
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ secrets.AWS_REGION }}

      # --------------------------
      # INSTALL AND CONFIGURE TOOLS
      # --------------------------
      - name: Install kubectl
        run: |
          curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"
          sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl
          kubectl version --client

      # --------------------------
      # FIX 1: DIRECT CLUSTER NAME SETUP
      # --------------------------
      - name: Set cluster name (OPTION 1 - Simple)
        run: |
          # Just set it directly - replace "eks" with your actual cluster name if different
          echo "CLUSTER_NAME=eks" >> $GITHUB_ENV
          echo "Using cluster name: eks"

          # Also set the correct region based on your setup
          # If your cluster is in us-east-1, make sure AWS_REGION is set correctly
          echo "AWS_REGION=us-east-1" >> $GITHUB_ENV

      # --------------------------
      # FIX 2: BETTER EXTRACTION FROM KUBECONFIG
      # --------------------------
      - name: Extract cluster info from kubeconfig (OPTION 2 - Automatic)
        if: false  # Set to true if you want automatic extraction
        env:
          KUBE_CONFIG_DATA: ${{ secrets.KUBE_CONFIG_DATA }}
        run: |
          echo "$KUBE_CONFIG_DATA" | base64 --decode > /tmp/kubeconfig.yaml

          # Method 1: Extract cluster name from server URL (most reliable)
          SERVER_URL=$(grep -m1 "server:" /tmp/kubeconfig.yaml | awk '{print $2}')
          echo "Server URL: $SERVER_URL"

          # Extract cluster name from EKS endpoint
          if [[ $SERVER_URL == *".eks.amazonaws.com"* ]]; then
            # Pattern: https://XXXXX.gr7.region.eks.amazonaws.com
            CLUSTER_NAME=$(echo $SERVER_URL | sed -E 's|.*//([^\.]+)\..*\.eks\.amazonaws\.com.*|\1|')
            echo "Extracted cluster name from URL: $CLUSTER_NAME"
          fi

          # Method 2: Try to get cluster name from current context
          if [ -z "$CLUSTER_NAME" ]; then
            CURRENT_CONTEXT=$(grep "current-context:" /tmp/kubeconfig.yaml | awk '{print $2}')
            echo "Current context: $CURRENT_CONTEXT"

            # Extract cluster name from ARN if present
            if [[ $CURRENT_CONTEXT == *"cluster/"* ]]; then
              CLUSTER_NAME=$(echo $CURRENT_CONTEXT | sed -E 's|.*cluster/([^/]+)|\1|')
              echo "Extracted cluster name from context: $CLUSTER_NAME"
            else
              CLUSTER_NAME=$CURRENT_CONTEXT
            fi
          fi

          # Clean up any ARN prefix
          CLUSTER_NAME=$(echo $CLUSTER_NAME | sed -E 's|^arn:aws:eks:[^:]+:[^:]+:cluster/||')

          echo "Final cluster name: $CLUSTER_NAME"
          echo "CLUSTER_NAME=$CLUSTER_NAME" >> $GITHUB_ENV

          # Save for debugging
          echo "=== KUBECONFIG CONTENTS ==="
          cat /tmp/kubeconfig.yaml

      # --------------------------
      # VERIFY AND SET CORRECT REGION
      # --------------------------
      - name: Verify AWS and cluster setup
        run: |
          # Verify AWS credentials
          echo "AWS Region: $AWS_REGION"
          aws sts get-caller-identity

          # List EKS clusters to see what's available
          echo "Listing EKS clusters in region $AWS_REGION..."
          aws eks list-clusters --region $AWS_REGION --output table

          # If cluster name is still wrong, you'll see it here
          # You can manually override if needed
          if [ -z "$CLUSTER_NAME" ] || [ "$CLUSTER_NAME" = "arn:aws:eks:"* ]; then
            echo "ERROR: Invalid cluster name detected: $CLUSTER_NAME"
            echo "Please set the correct cluster name in repository secrets as EKS_CLUSTER_NAME"
            exit 1
          fi

      # --------------------------
      # GENERATE KUBECONFIG
      # --------------------------
      - name: Generate kubeconfig for EKS
        run: |
          # Use cluster name from environment or secret
          if [ -n "${{ secrets.EKS_CLUSTER_NAME }}" ]; then
            echo "Using cluster name from secret"
            CLUSTER_NAME="${{ secrets.EKS_CLUSTER_NAME }}"
          fi

          echo "Using cluster: $CLUSTER_NAME in region: $AWS_REGION"

          # Generate kubeconfig
          aws eks update-kubeconfig \
            --name "$CLUSTER_NAME" \
            --region "$AWS_REGION" \
            --kubeconfig "$HOME/kubeconfig"

          echo "KUBECONFIG=$HOME/kubeconfig" >> $GITHUB_ENV

          # Verify
          echo "=== Generated kubeconfig ==="
          kubectl config view --kubeconfig="$HOME/kubeconfig"

      # --------------------------
      # TEST KUBERNETES CONNECTION
      # --------------------------
      - name: Test Kubernetes access
        run: |
          echo "Testing Kubernetes connection..."

          # Set retry logic
          MAX_RETRIES=3
          RETRY_COUNT=0

          while [ $RETRY_COUNT -lt $MAX_RETRIES ]; do
            echo "Attempt $((RETRY_COUNT + 1)) of $MAX_RETRIES"

            if kubectl cluster-info; then
              echo "✓ Successfully connected to Kubernetes!"
              break
            else
              RETRY_COUNT=$((RETRY_COUNT + 1))
              if [ $RETRY_COUNT -lt $MAX_RETRIES ]; then
                echo "✗ Connection failed, retrying in 5 seconds..."
                sleep 5
              else
                echo "✗ Failed to connect after $MAX_RETRIES attempts"
                exit 1
              fi
            fi
          done

          # Get cluster info
          echo "=== Cluster Information ==="
          kubectl get nodes
          kubectl get ns
          kubectl config current-context
          kubectl config view --minify

      # --------------------------
      # ALTERNATIVE: USE EXISTING KUBECONFIG (If you prefer)
      # --------------------------
      - name: Alternative - Use existing kubeconfig with fixes
        if: false  # Set to true if you want to use your existing kubeconfig
        env:
          KUBE_CONFIG_DATA: ${{ secrets.KUBE_CONFIG_DATA }}
        run: |
          # Write the kubeconfig
          echo "$KUBE_CONFIG_DATA" | base64 --decode > "$HOME/kubeconfig"

          # Fix the context if it's using ARN
          CURRENT_CONTEXT=$(kubectl config current-context --kubeconfig="$HOME/kubeconfig" 2>/dev/null || true)

          if [[ $CURRENT_CONTEXT == *"arn:aws:eks:"* ]]; then
            echo "Fixing context name..."
            # Extract just the cluster name
            NEW_CONTEXT=$(echo "$CURRENT_CONTEXT" | sed -E 's|arn:aws:eks:[^:]+:[^:]+:cluster/||')

            # Rename the context
            kubectl config rename-context "$CURRENT_CONTEXT" "$NEW_CONTEXT" --kubeconfig="$HOME/kubeconfig"

            # Set as current
            kubectl config use-context "$NEW_CONTEXT" --kubeconfig="$HOME/kubeconfig"
          fi

          echo "KUBECONFIG=$HOME/kubeconfig" >> $GITHUB_ENV

          # Test
          kubectl get nodes --request-timeout=10s

      - name: Ensure namespace exists
        run: |
          kubectl get ns ${{ env.NAMESPACE }} || kubectl create ns ${{ env.NAMESPACE }}

      # --------------------------
      # DEPLOY GREEN
      # --------------------------
      - name: Deploy GREEN version
        env:
          GREEN_IMAGE: ${{ secrets.REGISTRY }}/${{ secrets.IMAGE_NAMESPACE }}/green-app:${{ github.sha }}
        run: |
          sed -e "s|<GREEN_IMAGE>|${GREEN_IMAGE}|g" \
            k8s/deployments/green-deployment.yaml | kubectl apply -f -

      - name: Wait for GREEN rollout
        id: rollout
        run: |
          kubectl rollout status deployment/app-green \
            --namespace=${{ env.NAMESPACE }} \
            --timeout=180s

      # --------------------------
      # SMOKE TEST GREEN
      # --------------------------
      - name: Smoke test GREEN pod
        id: smoketest
        run: |
          set -e
          for i in {1..12}; do
            POD=$(kubectl get pods -n ${{ env.NAMESPACE }} \
              -l app=${{ env.APP_NAME }},version=${{ env.GREEN_LABEL }} \
              -o jsonpath="{.items[0].metadata.name}" || true)

            if [ -n "$POD" ]; then
              kubectl exec -n ${{ env.NAMESPACE }} $POD \
                -- curl -sSf http://localhost:5000/health && break
            fi

            echo "Waiting for GREEN pod readiness..."
            sleep 10
          done

      # --------------------------
      # FLIP SERVICE TO GREEN
      # --------------------------
      - name: Flip service to GREEN
        id: flip
        run: |
          kubectl -n ${{ env.NAMESPACE }} patch service app-service \
            -p '{"spec":{"selector":{"app":"blue-green-app","version":"green"}}}'

      # --------------------------
      # VERIFY GREEN SERVICE
      # --------------------------
      - name: Verify GREEN service
        id: verify
        run: |
          set -e
          for i in {1..10}; do
            kubectl get svc app-service -n ${{ env.NAMESPACE }}
            echo "Waiting for GREEN service response..."
            sleep 6

            POD=$(kubectl get pods -n ${{ env.NAMESPACE }} \
              -l app=${{ env.APP_NAME }},version=${{ env.GREEN_LABEL }} \
              -o jsonpath="{.items[0].metadata.name}" || true)

            if [ -n "$POD" ]; then
              kubectl exec -n ${{ env.NAMESPACE }} $POD \
                -- curl -sSf http://localhost:5000/health && exit 0
            fi
          done

          echo "GREEN service failed probe"
          exit 1


      # =====================================================================
      # AUTO-ROLLBACK if any step above failed
      # =====================================================================
      - name: AUTO-ROLLBACK to BLUE
        if: failure()
        run: |
          echo "⚠️ GREEN deployment failed — starting rollback..."

          echo "➡ Switching service back to BLUE"
          kubectl -n ${{ env.NAMESPACE }} patch service app-service \
            -p '{"spec":{"selector":{"app":"blue-green-app","version":"blue"}}}'

          echo "➡ Scaling GREEN to zero"
          kubectl scale deployment app-green --replicas=0 -n ${{ env.NAMESPACE }} || true

          echo "Rollback completed ❗ Deployment remains on BLUE"
          exit 1  # Mark workflow as failed so you know deployment failed


      # --------------------------
      # SCALE DOWN OLD BLUE
      # --------------------------
      - name: Scale down BLUE deployment
        if: success()
        run: |
          kubectl scale deployment app-blue --replicas=0 -n ${{ env.NAMESPACE }} || true

