name: CI/CD Blue-Green Deployment

on:
  push:
    branches:
      - main
      - 'release/*'
  workflow_dispatch:

env:
  APP_NAME: blue-green-app
  NAMESPACE: default
  GREEN_LABEL: green
  BLUE_LABEL: blue

jobs:

  # ========================================================================
  # BUILD AND SCAN (unchanged from your version)
  # ========================================================================
  build-and-scan:
    runs-on: ubuntu-latest
    outputs:
      blue_image: ${{ steps.push-blue.outputs.image }}
      green_image: ${{ steps.push-green.outputs.image }}

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: Install Bandit & deps
        run: |
          pip install --upgrade pip
          pip install bandit
          if [ -f app/blue/requirements.txt ]; then pip install -r app/blue/requirements.txt; fi
          if [ -f app/green/requirements.txt ]; then pip install -r app/green/requirements.txt; fi

      - name: Run Bandit Scan
        run: |
          bandit -r app/blue -ll || true
          bandit -r app/green -ll || true

      - name: Set up QEMU
        uses: docker/setup-qemu-action@v2

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v2

      - name: Login to Amazon ECR
        uses: aws-actions/amazon-ecr-login@v1
        with:
          region: us-east-1
        env:
          AWS_REGION: us-east-1
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}

      - name: Build & Push BLUE image
        id: push-blue
        uses: docker/build-push-action@v5
        with:
          context: ./app/blue
          file: ./app/blue/Dockerfile
          push: true
          tags: |
            ${{ secrets.ECR_REGISTRY }}/${{ secrets.ECR_REPOSITORY_BLUE }}:${{ github.sha }}
            ${{ secrets.ECR_REGISTRY }}/${{ secrets.ECR_REPOSITORY_BLUE }}:prod

      - name: Build & Push GREEN image
        id: push-green
        uses: docker/build-push-action@v5
        with:
          context: ./app/green
          file: ./app/green/Dockerfile
          push: true
          tags: |
            ${{ secrets.ECR_REGISTRY }}/${{ secrets.ECR_REPOSITORY_GREEN }}:${{ github.sha }}
            ${{ secrets.ECR_REGISTRY }}/${{ secrets.ECR_REPOSITORY_GREEN }}:prod

      - name: Trivy Scan (BLUE image)
        uses: aquasecurity/trivy-action@master
        with:
          image-ref: ${{ secrets.ECR_REGISTRY }}/${{ secrets.ECR_REPOSITORY_BLUE }}:${{ github.sha }}
          format: table
          exit-code: '1'
          severity: CRITICAL,HIGH

      - name: Trivy Scan (GREEN image)
        uses: aquasecurity/trivy-action@master
        with:
          image-ref: ${{ secrets.ECR_REGISTRY }}/${{ secrets.ECR_REPOSITORY_GREEN }}:${{ github.sha }}
          format: table
          exit-code: '1'
          severity: CRITICAL,HIGH

  # ========================================================================
  # DEPLOY, VERIFY, FLIP, AUTO-ROLLBACK
  # ========================================================================
  deploy-and-release:
    needs: build-and-scan
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main' || startsWith(github.ref, 'refs/heads/release/')
    env:
      AWS_REGION: ${{ secrets.AWS_REGION }}
      AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
      AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
      NAMESPACE: default
      APP_NAME: blue-green-app
      GREEN_LABEL: green
      BLUE_LABEL: blue

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v2
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ secrets.AWS_REGION }}

      # --------------------------
      # INSTALL AND CONFIGURE TOOLS
      # --------------------------
      - name: Install kubectl
        run: |
          curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"
          sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl
          kubectl version --client

      # --------------------------
      # AUTO-DETECT CLUSTER NAME
      # --------------------------
      - name: Detect EKS cluster name
        run: |
          echo "Listing EKS clusters in region $AWS_REGION..."

          # Get list of clusters
          CLUSTERS=$(aws eks list-clusters --region $AWS_REGION --query 'clusters' --output text)

          echo "Available clusters: $CLUSTERS"

          # Check if we have exactly one cluster
          if [ $(echo "$CLUSTERS" | wc -w) -eq 1 ]; then
            CLUSTER_NAME="$CLUSTERS"
            echo "Found single cluster: $CLUSTER_NAME"
          elif [ $(echo "$CLUSTERS" | wc -w) -gt 1 ]; then
            # Multiple clusters - check for specific names
            if echo "$CLUSTERS" | grep -q "bg-eks"; then
              CLUSTER_NAME="bg-eks"
              echo "Found 'bg-eks' cluster in list"
            elif echo "$CLUSTERS" | grep -q "\b${{ secrets.EKS_CLUSTER_NAME }}\b"; then
              CLUSTER_NAME="${{ secrets.EKS_CLUSTER_NAME }}"
              echo "Using cluster from secret: $CLUSTER_NAME"
            else
              # Take the first cluster
              CLUSTER_NAME=$(echo "$CLUSTERS" | awk '{print $1}')
              echo "Multiple clusters found. Using first one: $CLUSTER_NAME"
            fi
          else
            echo "ERROR: No EKS clusters found in region $AWS_REGION"
            echo "Available clusters: $CLUSTERS"
            exit 1
          fi

          echo "CLUSTER_NAME=$CLUSTER_NAME" >> $GITHUB_ENV
          echo "Using cluster: $CLUSTER_NAME"

      # --------------------------
      # VERIFY CLUSTER EXISTS
      # --------------------------
      - name: Verify cluster access
        run: |
          echo "Verifying access to cluster: $CLUSTER_NAME"

          # Check if cluster exists and is accessible
          aws eks describe-cluster \
            --name "$CLUSTER_NAME" \
            --region "$AWS_REGION" \
            --query 'cluster.status' \
            --output text

          echo "✓ Cluster $CLUSTER_NAME is accessible"

      # --------------------------
      # GENERATE KUBECONFIG
      # --------------------------
      - name: Generate EKS kubeconfig
        run: |
          echo "Generating kubeconfig for cluster: $CLUSTER_NAME"

          # Generate kubeconfig
          aws eks update-kubeconfig \
            --name "$CLUSTER_NAME" \
            --region "$AWS_REGION" \
            --kubeconfig "$HOME/kubeconfig"

          # Set environment variable
          echo "KUBECONFIG=$HOME/kubeconfig" >> $GITHUB_ENV

          # Verify kubeconfig
          echo "=== Generated kubeconfig ==="
          kubectl config view --kubeconfig="$HOME/kubeconfig" --minify

      # --------------------------
      # TEST KUBERNETES CONNECTION
      # --------------------------
      - name: Test Kubernetes connectivity
        run: |
          echo "Testing connection to Kubernetes cluster..."

          # Test with retry logic
          for i in {1..3}; do
            echo "Attempt $i/3"
            if kubectl cluster-info --request-timeout=10s; then
              echo "✓ Successfully connected to Kubernetes"
              break
            else
              if [ $i -eq 3 ]; then
                echo "✗ Failed to connect after 3 attempts"
                exit 1
              fi
              echo "Waiting 5 seconds before retry..."
              sleep 5
            fi
          done

          # Get cluster details
          echo ""
          echo "=== Cluster Details ==="
          kubectl get nodes
          kubectl get ns

          echo ""
          echo "=== Current Context ==="
          kubectl config current-context

          echo ""
          echo "=== Cluster Info ==="
          kubectl cluster-info

      # --------------------------
      # OPTIONAL: DEPLOYMENT STEPS
      # --------------------------
      - name: Deploy application (example)
        run: |
          echo "Cluster is ready for deployment!"
          echo "Cluster: $CLUSTER_NAME"
          echo "Region: $AWS_REGION"
          echo "Namespace: $NAMESPACE"

          # Your deployment commands will go here
          # For example:
          # kubectl apply -f k8s/deployment.yaml
          # kubectl apply -f k8s/service.yaml
          # kubectl apply -f k8s/ingress.yaml

      - name: Ensure namespace exists
        run: |
          kubectl get ns ${{ env.NAMESPACE }} || kubectl create ns ${{ env.NAMESPACE }}

      # --------------------------
      # DEPLOY GREEN
      # --------------------------
      - name: Deploy GREEN version
        env:
          GREEN_IMAGE: ${{ secrets.REGISTRY }}/${{ secrets.IMAGE_NAMESPACE }}/green-app:${{ github.sha }}
        run: |
          sed -e "s|<GREEN_IMAGE>|${GREEN_IMAGE}|g" \
            k8s/deployments/green-deployment.yaml | kubectl apply -f -

      - name: Wait for GREEN rollout
        id: rollout
        run: |
          kubectl rollout status deployment/app-green \
            --namespace=${{ env.NAMESPACE }} \
            --timeout=180s

      # --------------------------
      # SMOKE TEST GREEN
      # --------------------------
      - name: Smoke test GREEN pod
        id: smoketest
        run: |
          set -e
          for i in {1..12}; do
            POD=$(kubectl get pods -n ${{ env.NAMESPACE }} \
              -l app=${{ env.APP_NAME }},version=${{ env.GREEN_LABEL }} \
              -o jsonpath="{.items[0].metadata.name}" || true)

            if [ -n "$POD" ]; then
              kubectl exec -n ${{ env.NAMESPACE }} $POD \
                -- curl -sSf http://localhost:5000/health && break
            fi

            echo "Waiting for GREEN pod readiness..."
            sleep 10
          done

      # --------------------------
      # FLIP SERVICE TO GREEN
      # --------------------------
      - name: Flip service to GREEN
        id: flip
        run: |
          kubectl -n ${{ env.NAMESPACE }} patch service app-service \
            -p '{"spec":{"selector":{"app":"blue-green-app","version":"green"}}}'

      # --------------------------
      # VERIFY GREEN SERVICE
      # --------------------------
      - name: Verify GREEN service
        id: verify
        run: |
          set -e
          for i in {1..10}; do
            kubectl get svc app-service -n ${{ env.NAMESPACE }}
            echo "Waiting for GREEN service response..."
            sleep 6

            POD=$(kubectl get pods -n ${{ env.NAMESPACE }} \
              -l app=${{ env.APP_NAME }},version=${{ env.GREEN_LABEL }} \
              -o jsonpath="{.items[0].metadata.name}" || true)

            if [ -n "$POD" ]; then
              kubectl exec -n ${{ env.NAMESPACE }} $POD \
                -- curl -sSf http://localhost:5000/health && exit 0
            fi
          done

          echo "GREEN service failed probe"
          exit 1


      # =====================================================================
      # AUTO-ROLLBACK if any step above failed
      # =====================================================================
      - name: AUTO-ROLLBACK to BLUE
        if: failure()
        run: |
          echo "⚠️ GREEN deployment failed — starting rollback..."

          echo "➡ Switching service back to BLUE"
          kubectl -n ${{ env.NAMESPACE }} patch service app-service \
            -p '{"spec":{"selector":{"app":"blue-green-app","version":"blue"}}}'

          echo "➡ Scaling GREEN to zero"
          kubectl scale deployment app-green --replicas=0 -n ${{ env.NAMESPACE }} || true

          echo "Rollback completed ❗ Deployment remains on BLUE"
          exit 1  # Mark workflow as failed so you know deployment failed


      # --------------------------
      # SCALE DOWN OLD BLUE
      # --------------------------
      - name: Scale down BLUE deployment
        if: success()
        run: |
          kubectl scale deployment app-blue --replicas=0 -n ${{ env.NAMESPACE }} || true

